name: Download Passwall2 IPKs (HTML Scraper)

on:
  workflow_dispatch:

jobs:
  download:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Dependencies
        run: pip install requests beautifulsoup4

      - name: Scrape and Download
        run: |
          mkdir -p output
          
          # 创建 Python 脚本
          cat << 'EOF' > scraper.py
          import requests
          from bs4 import BeautifulSoup
          import os
          import urllib.parse

          # 1. 配置
          # 这就是你给的那个“肉眼可见”的网页地址
          PAGE_URL = "https://sourceforge.net/projects/openwrt-passwall-build/files/releases/packages-21.02/aarch64_cortex-a53/passwall_packages/"
          
          # 对应的真实下载前缀 (SourceForge 的规律: /files/ -> /project/ 且域名变 downloads)
          DOWNLOAD_BASE = "https://downloads.sourceforge.net/project/openwrt-passwall-build/releases/packages-21.02/aarch64_cortex-a53/passwall_packages"

          print(f"正在扫描页面: {PAGE_URL}")

          # 2. 获取网页内容
          headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
          try:
              resp = requests.get(PAGE_URL, headers=headers)
              resp.raise_for_status()
          except Exception as e:
              print(f"网页访问失败: {e}")
              exit(1)

          # 3. 解析网页中的所有 IPK 文件名
          soup = BeautifulSoup(resp.text, 'html.parser')
          available_files = []
          
          # SourceForge 的文件列表通常在 tr.file > th > a 标签中
          for a_tag in soup.find_all('a'):
              href = a_tag.get('href', '')
              title = a_tag.get('title', '')
              
              # 提取文件名：如果是 .ipk 结尾
              if title.endswith('.ipk'):
                  available_files.append(title)
              elif href.endswith('.ipk/download'): # 备用匹配逻辑
                  name = href.split('/')[-2]
                  available_files.append(name)

          # 去重
          available_files = list(set(available_files))
          print(f"页面上共找到 {len(available_files)} 个 IPK 文件。")
          # print(available_files) # 调试用

          # 4. 读取你的需求清单
          if not os.path.exists("packages.txt"):
              print("错误：找不到 packages.txt")
              exit(1)
              
          with open("packages.txt", 'r') as f:
              targets = [line.strip() for line in f if line.strip() and not line.startswith("#")]

          # 5. 匹配并下载
          downloaded_count = 0
          
          for keyword in targets:
              print(f"\n--- 寻找: {keyword} ---")
              
              # 模糊匹配：只要文件名包含关键词
              matches = [f for f in available_files if keyword in f]
              
              if not matches:
                  print(f"   未找到包含 '{keyword}' 的文件")
                  continue
                  
              # 如果有多个匹配，优先选最短的（通常最精准），或者选包含版本号最新的
              # 这里简单粗暴：选第一个包含该关键词的
              target_file = matches[0]
              
              # 特殊处理 shadowsocks-rust：确保下对
              if keyword == "shadowsocks-rust":
                  # 优先找 ss-local 或 ss-server 没意义，通常有一个主包或者包含所有
                  pass

              print(f"   锁定文件: {target_file}")
              
              # 构造下载链接
              download_url = f"{DOWNLOAD_BASE}/{target_file}"
              save_path = os.path.join("output", target_file)
              
              print(f"   正在下载...")
              try:
                  r = requests.get(download_url, headers=headers, stream=True)
                  r.raise_for_status()
                  with open(save_path, 'wb') as f:
                      for chunk in r.iter_content(chunk_size=8192):
                          f.write(chunk)
                  print("   下载成功！")
                  downloaded_count += 1
              except Exception as e:
                  print(f"   下载失败: {e}")

          print(f"\n任务完成！共下载 {downloaded_count} 个文件。")
          if downloaded_count == 0:
              exit(1)
          
          EOF
          
          python3 scraper.py

      - name: Repackage
        run: |
          cd output
          zip -r ../passwall_ipk_ready.zip .

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: passwall_ipk_ready
          path: passwall_ipk_ready.zip
